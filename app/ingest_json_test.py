#!/usr/bin/env python3
"""Ingestion d'un JSON local (response_1.json) vers HDFS + archivage local."""

import os
import json
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp, lit

# Configuration (surcharg√©e via variables d'environnement au besoin)
HDFS_NAMENODE = os.getenv("HDFS_NAMENODE", "namenode")
HDFS_PORT = os.getenv("HDFS_PORT", "9000")
HDFS_PATH = os.getenv("HDFS_PATH", "/users/ipssi/input/binance_batch")

JSON_FILE_PATH = "/app/response_1.json"  # Chemin dans le conteneur (sans espace)
ARCHIVE_PATH = "/app/data/raw/response_1_archived.json"

def create_spark_session():
    """Cr√©e une session Spark connect√©e au cluster (master d√©fini par env)."""
    spark_master = os.getenv("SPARK_MASTER_HOST", "spark-master")
    spark_port = os.getenv("SPARK_MASTER_PORT", "7077")
    
    return (
        SparkSession.builder
        .appName("BinanceTestJSONIngestion")
        .master(f"spark://{spark_master}:{spark_port}")
        .config("spark.hadoop.fs.defaultFS", f"hdfs://{HDFS_NAMENODE}:{HDFS_PORT}")
        .getOrCreate()
    )

def ingest_json_to_hdfs():
    """Ing√®re le fichier JSON de test dans HDFS avec timestamp et m√©tadonn√©es."""
    print(f"[{datetime.now()}] === Ingestion du fichier JSON de test ===")

    if not os.path.exists(JSON_FILE_PATH):
        print(f"[ERROR] Fichier {JSON_FILE_PATH} non trouv√©!")
        return False

    # Charger le fichier en local pour √©viter les probl√®mes d'acc√®s des executors
    try:
        with open(JSON_FILE_PATH, "r", encoding="utf-8") as f:
            raw = f.read().rstrip()
            # Certains dumps contiennent un '%' final : on le retire si pr√©sent
            if raw.endswith("%"):
                raw = raw[:-1]
            data = json.loads(raw)
    except Exception as e:
        print(f"[ERROR] Lecture/parse du JSON √©chou√©e: {e}")
        return False

    if not isinstance(data, list) or not data:
        print("[ERROR] Le contenu JSON n'est pas une liste non vide")
        return False

    spark = None  # initialis√© ici pour pouvoir le stopper en finally
    try:
        spark = create_spark_session()

        # Cr√©ation du DataFrame √† partir des donn√©es locales (pas de lecture distribu√©e)
        print(f"[{datetime.now()}] Cr√©ation du DataFrame depuis {len(data)} enregistrements")
        df = spark.createDataFrame(data)

        # Ajouter m√©tadonn√©es
        df_enriched = df \
            .withColumn("timestamp_ingestion", current_timestamp()) \
            .withColumn("batch_date", lit(datetime.now().strftime("%Y-%m-%d"))) \
            .withColumn("source", lit("test_response_1_json"))

        # Cr√©er le dossier avec timestamp (suffixe _test pour tracer la source)
        batch_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        hdfs_full_path = f"hdfs://{HDFS_NAMENODE}:{HDFS_PORT}{HDFS_PATH}/{batch_timestamp}_test"

        print(f"[{datetime.now()}] √âcriture vers HDFS : {hdfs_full_path}")
        df_enriched.write \
            .mode("overwrite") \
            .option("header", "true") \
            .csv(hdfs_full_path)

        print(f"[{datetime.now()}] ‚úÖ Ingestion r√©ussie : {df_enriched.count()} lignes √©crites")
        return True

    except Exception as e:
        print(f"[ERROR] √âchec de l'ingestion : {str(e)}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        if spark:
            spark.stop()

def archive_json_file():
    """Archive le fichier JSON apr√®s ingestion (copie + suppression source)."""
    try:
        # Cr√©er le r√©pertoire d'archive s'il n'existe pas
        os.makedirs(os.path.dirname(ARCHIVE_PATH), exist_ok=True)
        
        # Copier le fichier
        import shutil
        shutil.copy2(JSON_FILE_PATH, ARCHIVE_PATH)
        
        print(f"[{datetime.now()}] ‚úÖ Fichier archiv√© : {ARCHIVE_PATH}")
        
        # Supprimer l'original
        os.remove(JSON_FILE_PATH)
        print(f"[{datetime.now()}] ‚úÖ Fichier original supprim√© : {JSON_FILE_PATH}")
        
        return True
        
    except Exception as e:
        print(f"[ERROR] √âchec de l'archivage : {str(e)}")
        return False

if __name__ == "__main__":
    print(f"\n{'='*60}")
    print(f"[{datetime.now()}] D√©marrage de l'ingestion JSON de test")
    print(f"{'='*60}\n")
    
    # √âtape 1 : Ing√©rer dans HDFS
    if ingest_json_to_hdfs():
        # √âtape 2 : Archiver le fichier
        archive_json_file()
        print(f"\n[{datetime.now()}] üéâ Processus termin√© avec succ√®s!")
    else:
        print(f"\n[{datetime.now()}] ‚ùå √âchec du processus")
        exit(1)
